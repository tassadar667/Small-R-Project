---
title: "2021年4月13日 贝叶斯作业"
author: "tassadar667"
date: "2021/4/13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 利用SGD对鸢尾花种类进行逻辑回归
SGD函数
```{r}
sgd.lm <- function(X, y, beta.init, alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10) {
  n <- length(y)
  beta.old <- beta.init
  J <- betas <- list()
  sto.sample <- sample(1:n, n.samples, replace = TRUE)
  betas[[1]] <- beta.old
  J[[1]] <- lm.cost(X, y, beta.old)
  beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
  betas[[2]] <- beta.new
  J[[2]] <- lm.cost(X, y, beta.new)
  iter <- 0
  n.best <- 0
  while ((abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol) & (iter + 2 < max.iter)) {
    beta.old <- beta.new
    sto.sample <- sample(1:n, n.samples, replace = TRUE)
    beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
    iter <- iter + 1
    betas[[iter + 2]] <- beta.new
    J[[iter + 2]] <- lm.cost(X, y, beta.new)
  }
  if (abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol) {
    cat("Did not converge. \n")
  } else {
    cat("Converged. \n")
    cat("Iterated ", iter + 1, " times.", "\n")
    cat("Coefficients: ", beta.new, "\n")
    return(list(coef = betas, cost = J, niter = iter + 1,result=beta.new))
  }
}
## Make the cost function
sgd.lm.cost <- function(X, y, beta) {
  n <- length(y)
  if (!is.matrix(X)) {
    X <- matrix(X, nrow = 1)
  }
  loss <- sum((sig(X %*% beta) - y)^2)/(2 * n)
  return(loss)
}
## Calculate the gradient
sgd.lm.cost.grad <- function(X, y, beta) {
  n <- length(y)
  if (!is.matrix(X)) {
    X <- matrix(X, nrow = 1)
  }
  t(X) %*% (X %*% beta - y)/n
}

lm.cost <- function(X, y, beta) {
  n <- length(y)
  loss <- sum((X %*% beta - y)^2)/(2 * n)
  return(loss)
}
## Calculate the gradient
lm.cost.grad <- function(X, y, beta) {
  n <- length(y)
  return()
}

```
给出逻辑回归函数
```{r}
sig<-function(x){
  1/(1+exp(-x))
}
```
初始化数据,数据为鸢尾花数据集，包含sepal长度宽度，petal长度宽度，预测变量为该花是否为setosa
```{r}
data=read.csv('iris.csv')
head(data)
x=data[,c(1,2,3,4)]
y=data[,5]
x=as.matrix(x)
x=scale(x)
y=as.matrix(y)
init_beta=matrix(rnorm(4),ncol = 1)
```
取样计算
```{r}
s=sample(1:150,100)
s_x=x[s,]
s_y=y[s,]

res=sgd.lm(s_x,s_y,init_beta,alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10)
```
对预测结果进行检验
```{r}
yy=sig(x%*%res$result)
pred1=c()
for (i in yy) {
  if(i>=0.5){
    pred1=c(pred1,1)
  }
  else{
    pred1=c(pred1,0)
  }
}
right=0
for (i in 1:150) {
  if(pred1[i]==y[i]){
    right=right+1
  }
}
cat("准确率为",right/150)
```
可以看到预测准确率较高，模型表现良好

## 小结
对于逻辑回归这种较为简单的模型，有着较好的数学性质（可导、单调），随机梯度下降难以体现其优势，但是对于更复杂的模型，随机梯度下降能够较为快的解决问题。
