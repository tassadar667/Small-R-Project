---
title: "2021年4月13日 贝叶斯作业"
author: "tassadar667"
date: "2021/4/13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 利用SGD对鸢尾花种类进行逻辑回归
SGD函数
```{r}
sgd.lm <- function(X, y, beta.init, alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10) {
  n <- length(y)
  beta.old <- beta.init
  J <- betas <- list()
  sto.sample <- sample(1:n, n.samples, replace = TRUE)
  betas[[1]] <- beta.old
  J[[1]] <- lm.cost(X, y, beta.old)
  beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
  betas[[2]] <- beta.new
  J[[2]] <- lm.cost(X, y, beta.new)
  iter <- 0
  n.best <- 0
  while ((abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol) & (iter + 2 < max.iter)) {
    beta.old <- beta.new
    sto.sample <- sample(1:n, n.samples, replace = TRUE)
    beta.new <- beta.old - alpha * sgd.lm.cost.grad(X[sto.sample, ], y[sto.sample], beta.old)
    iter <- iter + 1
    betas[[iter + 2]] <- beta.new
    J[[iter + 2]] <- lm.cost(X, y, beta.new)
  }
  if (abs(lm.cost(X, y, beta.new) - lm.cost(X, y, beta.old)) > tol) {
    cat("Did not converge. \n")
  } else {
    cat("Converged. \n")
    cat("Iterated ", iter + 1, " times.", "\n")
    cat("Coefficients: ", beta.new, "\n")
    return(list(coef = betas, cost = J, niter = iter + 1,result=beta.new))
  }
}
## Make the cost function
sgd.lm.cost <- function(X, y, beta) {
  n <- length(y)
  if (!is.matrix(X)) {
    X <- matrix(X, nrow = 1)
  }
  loss <- sum((sig(X %*% beta) - y)^2)/(2 * n)
  return(loss)
}
## Calculate the gradient
sgd.lm.cost.grad <- function(X, y, beta) {
  n <- length(y)
  if (!is.matrix(X)) {
    X <- matrix(X, nrow = 1)
  }
  t(X) %*% (X %*% beta - y)/n
}

lm.cost <- function(X, y, beta) {
  n <- length(y)
  loss <- sum((X %*% beta - y)^2)/(2 * n)
  return(loss)
}
## Calculate the gradient
lm.cost.grad <- function(X, y, beta) {
  n <- length(y)
  return()
}

```
给出逻辑回归函数
```{r}
sig<-function(x){
  1/(1+exp(-x))
}
```
初始化数据,数据为鸢尾花数据集，包含sepal长度宽度，petal长度宽度，预测变量为该花是否为setosa
建立函数为$y=\frac{1}{1+e^{-x}}$，其中$x=\beta_1sepal.length+\beta_2sepal.width+\beta_3petal.length+\beta_4petal.width+\beta_5$
```{r}
data=read.csv('iris.csv')
head(data)
x=data[,c(1,2,3,4)]
y=data[,5]
x=as.matrix(x)
x=scale(x)
x=cbind(x,rep(1,150))
y=as.matrix(y)
init_beta=matrix(rnorm(5),ncol = 1)
```
取样计算
```{r}
s=sample(1:150,100)
s_x=x[s,]
s_y=y[s,]

res=sgd.lm(s_x,s_y,init_beta,alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10)
```
对预测结果进行检验
```{r}
yy=sig(x%*%res$result)
pred1=c()
for (i in yy) {
  if(i>=0.5){
    pred1=c(pred1,1)
  }
  else{
    pred1=c(pred1,0)
  }
}
right=0
for (i in 1:150) {
  if(pred1[i]==y[i]){
    right=right+1
  }
}
cat("准确率为",right/150)
y1=yy
```
可以看到预测准确率较低，对比直接逻辑回归结果
```{r}
ans=glm.fit(s_x,s_y)
print(ans$coefficients)
```
两者系数较为相似，考虑对三种类型均进行逻辑回归，选择其中可能性最大的输出
```{r}
y=data[,6]
y=as.matrix(y)
s_y=y[s,]
init_beta=matrix(rnorm(5),ncol = 1)
res=sgd.lm(s_x,s_y,init_beta,alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10)
```
```{r}
yy=sig(x%*%res$result)
pred1=c()
for (i in yy) {
  if(i>=0.5){
    pred1=c(pred1,1)
  }
  else{
    pred1=c(pred1,0)
  }
}
right=0
for (i in 1:150) {
  if(pred1[i]==y[i]){
    right=right+1
  }
}
cat("准确率为",right/150)
y2=yy
```

```{r}
y=data[,7]
y=as.matrix(y)
s_y=y[s,]
init_beta=matrix(rnorm(5),ncol = 1)
res=sgd.lm(s_x,s_y,init_beta,alpha = 0.05, n.samples = 1, tol = 1e-8, max.iter = 1e10)
```
```{r}
yy=sig(x%*%res$result)
pred1=c()
for (i in yy) {
  if(i>=0.5){
    pred1=c(pred1,1)
  }
  else{
    pred1=c(pred1,0)
  }
}
right=0
for (i in 1:150) {
  if(pred1[i]==y[i]){
    right=right+1
  }
}
cat("准确率为",right/150)
y3=yy
```
```{r}
y=data[,c(5,6,7)]
right=0
for (i in 1:150) {
  if(which.max(c(y1[i],y2[i],y3[i]))==which.max(y[i,])){
    right=right+1
  }
  
}
cat("准确率为",right/150)
```
使用“投票”的方法增大了准确率。


## 小结
对于逻辑回归这种较为简单的模型，有着较好的数学性质（可导、单调），随机梯度下降难以体现其优势，但是对于更复杂的模型，随机梯度下降能够较为快的解决问题。
